// Module included in the following assemblies:
//
// * documentation/doc-Migration_Toolkit_for_Virtualization/master.adoc

[id="migrating-virtual-machines-cli_{context}"]
= Migrating virtual machines from the command line interface

You can migrate virtual machines (VMs) from the command line (CLI) by creating the following custom resources (CRs):

* `Secret` contains the source provider credentials.
* `Provider` contains the source provider details.
* `Host` contains the VMware host details.
* `NetworkMap` maps the source and destination networks.
* `StorageMap` maps the source and destination storage.
* `Plan` contains a list of VMs to migrate and specifies whether the migration is cold or warm. The `Plan` references the providers and maps.
* `Migration` runs the `Plan`. If the migration is warm, it specifies the cutover time.
+
You can associate multiple `Migration` CRs with a single `Plan` CR. If a migration does not complete, you can create a new `Migration` CR, without changing the `Plan` CR, to migrate the remaining VMs.

The term _destination_ in the API is the same as _target_ in the web console.

[IMPORTANT]
====
You must specify a name for cluster-scoped CRs.

You must specify both a name and a namespace for namespace-scoped CRs.
====

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges.
* The link:https://docs.openshift.com/container-platform/{ocp-version}/cli_reference/openshift_cli/getting-started-cli.html[OpenShift CLI] must be installed.
* If you are mapping more than one source and destination network, you must create a link:https://docs.openshift.com/container-platform/{ocp-version}/virt/virtual_machines/vm_networking/virt-attaching-vm-multiple-networks.html#virt-creating-network-attachment-definition[network attachment definition] for each additional destination network.
* If your source provider is VMware vSphere, the following additional prerequisites apply:
** The VDDK image must be added to the `HyperConverged` custom resource.
** For warm migration, you must enable link:https://kb.vmware.com/s/article/1020128[changed block tracking (CBT)] on the VMs and on the VM disks.
** If you are performing more than 10 concurrent migrations from a single ESXi host, you must increase the NFC service memory of the host.
** You must have the SHA-1 fingerprint of the vCenter host. You can obtain the fingerprint by running the following command:
+
[source,terminal]
----
$ openssl s_client \
    -connect <www.example.com>:443 \ <1>
    < /dev/null 2>/dev/null \
    | openssl x509 -fingerprint -noout -in /dev/stdin \
    | cut -d '=' -f 2
----
<1> Specify the vCenter name.
+
.Example output
+
[source,terminal]
----
01:23:45:67:89:AB:CD:EF:01:23:45:67:89:AB:CD:EF:01:23:45:67
----

.Procedure

. Create a `Secret` CR manifest:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <secret>
  namespace: {namespace}
type: Opaque
data:
  user: <user> <1>
  password: <password> <2>
  cacert: <{rhv_short}_ca_certificate> <3>
  thumbprint: <vcenter_fingerprint> <4>
EOF
----
<1> Specify the base64-encoded vCenter admin user or the {rhv-short} {manager} user.
<2> Specify the base64-encoded password.
<3> {rhv-short} only: Specify the base64-encoded CA certificate of the {manager}. You can retrieve it at `https://<www.example.com>/ovirt-engine/services/pki-resource?resource=ca-certificate&format=X509-PEM-CA`.
<4> VMware only: Specify the vCenter SHA-1 fingerprint.

. Create a `Provider` CR manifest:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Provider
metadata:
  name: <provider>
  namespace: {namespace}
spec:
  type: <provider_type> <1>
  url: <api_end_point> <2>
  secret:
    name: <secret> <3>
    namespace: {namespace}
EOF
----
<1> Allowed values are `ovirt` and `vsphere`.
<2> Specify the API end point URL, for example, `https://<www.example.com>/sdk` for vSphere or `https://<www.example.com>/ovirt-engine/api/` for {rhv-short}.
<3> Specify the name of provider `Secret` CR.

. VMware only: Create a `Host` CR manifest:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Host
metadata:
  name: <vmware_host>
  namespace: {namespace}
spec:
  provider:
    namespace: {namespace}
    name: <source_provider> <1>
  id: <source_host_mor> <2>
  ipAddress: <source_network_ip> <3>
EOF
----
<1> Specify the name of the VMware `Provider` CR.
<2> Specify the VMware host MOR.
<3> Specify the IP address of the VMware migration network.

. Create a `NetworkMap` CR manifest to map the source and destination networks:
+
[source,yaml,subs="attributes+"]
----
$  cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: NetworkMap
metadata:
  name: <network_map>
  namespace: {namespace}
spec:
  map:
    - destination:
        name: <pod>
        namespace: {namespace}
        type: pod <1>
      source: <2>
        id: <source_network_id> <3>
        name: <source_network_name>
    - destination:
        name: <network_attachment_definition> <4>
        namespace: <network_attachment_definition_namespace> <5>
        type: multus
      source:
        id: <source_network_id>
        name: <source_network_name>
  provider:
    source:
      name: <source_provider>
      namespace: {namespace}
    destination:
      name: <destination_cluster>
      namespace: {namespace}
EOF
----
<1> Allowed values are `pod` and `multus`.
<2> You can use either the `id` _or_ the `name` parameter to specify the source network.
<3> Specify a VMware network MOR or {a-rhv-short} network ID.
<4> Specify a network attachment definition for each additional {virt} network.
<5> Specify the namespace of the {virt} network attachment definition.

. Create a `StorageMap` CR manifest:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: StorageMap
metadata:
  name: <storage_map>
  namespace: {namespace}
spec:
  map:
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode> <1>
      source:
        id: <source_datastore> <2>
    - destination:
        storageClass: <storage_class>
        accessMode: <access_mode>
      source:
        id: <source_datastore>
  provider:
    source:
      name: <source_provider>
      namespace: {namespace}
    destination:
      name: <destination_cluster>
      namespace: {namespace}
EOF
----
<1> Allowed values are `ReadWriteOnce` and `ReadWriteMany`.
<2> Specify a VMware data storage MOR or {a-rhv-short} storage domain ID, for example, `f2737930-b567-451a-9ceb-2887f6207009`.

. Create a `Plan` CR manifest for the migration:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Plan
metadata:
  name: <plan> <1>
  namespace: {namespace}
spec:
  warm: true <2>
  provider:
    source:
      name: <source_provider>
      namespace: {namespace}
    destination:
      name: <destination_cluster>
      namespace: {namespace}
  map:
    network: <3>
      name: <network_map> <4>
      namespace: {namespace}
    storage:
      name: <storage_map> <5>
      namespace: {namespace}
  targetNamespace: {namespace}
  vms: <6>
    - id: <source_vm> <7>
    - name: <source_vm>
EOF
----
<1> Specify the name of the `Plan` CR.
<2> VMware only: Specify whether the migration is warm or cold. If you specify a warm migration without specifying a value for the `cutover` parameter in the `Migration` CR manifest, only the precopy stage will run. Warm migration is not supported for {rhv-short}.
<3> You can add multiple network mappings.
<4> Specify the name of the `NetworkMap` CR.
<5> Specify the name of the `StorageMap` CR.
<6> You can use either the `id` _or_ the `name` parameter to specify the source VMs.
<7> Specify a VMware VM MOR or {a-rhv-short} VM ID.

. Optional and for VMware only: To change the time interval between the CBT snapshots for warm migration, patch the `vm-import-controller-config` config map:
+
[source,terminal]
----
$ oc patch configmap/vm-import-controller-config \
  -n openshift-cnv -p '{"data": \
  {"warmImport.intervalMinutes": "<interval>"}}' <1>
----
<1> Specify the time interval in minutes. The default value is `60`.

. Create a `Migration` CR manifest to run the `Plan` CR:
+
[source,yaml,subs="attributes+"]
----
$ cat << EOF | oc apply -f -
apiVersion: forklift.konveyor.io/v1beta1
kind: Migration
metadata:
  name: <migration> <1>
  namespace: {namespace}
spec:
  plan:
    name: <plan> <2>
    namespace: {namespace}
  cutover: <cutover_time> <3>
EOF
----
<1> Specify the name of the `Migration` CR.
<2> Specify the name of the `Plan` CR that you are running. The `Migration` CR creates a `VirtualMachineImport` CR for each VM that is migrated.
<3> Optional: Specify a cutover time according to the ISO 8601 format with the UTC time offset, for example, `2021-04-04T01:23:45.678+09:00`.

. View the `VirtualMachineImport` pods to monitor the progress of the migration:
+
[source,terminal,subs="attributes+"]
----
$ oc get pods -n {namespace}
----
